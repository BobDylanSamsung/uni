## Question 1

Let $det(F_n) = (a + ib) \in \mathbb{C}$ 

It is known $F_n\overline{F_n} = nI$ 

Thus: 
$$\begin{equation}\begin{aligned}
det(nI) = n&= det(F_n\overline{F_n})\\
&= det(F_n) \space det(\overline{F_n})\\
&=det(F_n) \space \overline{det(F_n)}\\
&= (a + ib)(a-ib)\\
&= a^2 + b^2 - 2abi\\
\end{aligned}\end{equation}
$$
$$\begin{align}
Thus:\\
&\implies a^2 + b^2 = n \space and \space -2abi=0\\
&\implies ab=0\\
&\implies a=0\space or \space b=0\\
&\implies a = \pm\sqrt{n} \text{ if b=0 or } b=\pm\sqrt{n}\text{ if a=0}\\
&\implies det(F_n) = \pm\sqrt{n} \space or \space \pm\sqrt{n}i\\\\
As\space |det(F_n)| = \sqrt{a^2 + b^2}:\\
&|det(F_n)| = \sqrt{(\pm\sqrt{n})^2} = \sqrt{n}\\
\end{align}
$$

## Question 2
#### Part a
$$
\begin{array}
{c|cc}
0&0&0\\
\frac{1}{2}&\frac{1}{2}&0\\
\hline
&0&1
\end{array}
$$

#### Part b
Consider:
$$\begin{align}
\frac{du}{dt}=f(t,u)&= \lambda u\\
\implies u_{k+1} &= u_k+h\lambda(u_k+\dfrac{h}{2}f(t_k,u_k))\\
&=u_k+h\lambda(u_k+\frac{h}{2}\lambda u_k)\\
&= u_k + h\lambda u_k + \frac{(h\lambda)^2}{2}u_k\\
&= \rho(h\lambda)u_k \\
where \space \rho(z)&=(1 + z + \frac{z^2}{2}) \text{ is the amplification factor}
\end{align}$$
#### Part c
For $u_k \longrightarrow 0$: $\lvert \rho(h\lambda)\rvert < 1$
That is: 
$$\begin{aligned}
|1 + h\lambda + \frac{(h\lambda)^2}{2} | &< 1\\
\implies 1+h\lambda+\frac{(h\lambda)^2}{2} &< 1\\
\implies h\lambda + \frac{(h\lambda)^2}{2}&<0\\
\implies h\lambda(1 + \frac{1}{2}h\lambda)&<0
\end{aligned}$$
Now: 
$$
\begin{aligned}
f(t,u) = -5u&\implies \lambda = -5\\
\implies 0 &>-5h(1-\frac{5}{2}h)\\
\implies 0 &> h(\frac{5}{2}h -1 )\\
\implies h &\in (0, \frac 2 5)
\end{aligned}$$
Thus the smallest value of $h$ such that $u_k$ does not converge to 0 is $h = \frac 2 5 \implies N = 4$ 

## Question 3
#### Part a
$$
\newcommand{\QQQ}{&\qquad\qquad\qquad&}
\newcommand{\ii}{\space\implies}
\begin{align}
&x'(t) = -sin(t) \QQQ y'(t)=cos(t)\\
&\ii x(t) = cos(t) + c_x \QQQ \ii y(t) = sin(t) + c_y\\
\\
&x(0) = 1 \QQQ y(0) = 0\\
&\ii cos(0) + c_x = 1 \QQQ\ii sin(0) + c_y = 0\\
&\ii c_x = 0 \QQQ \ii c_y = 0\\
&\ii x(t)=cos(t) \QQQ \ii y(t) = sin(t)\\
\\Thus:\\
&P(t) = 
\begin{bmatrix}
           cos(t) \\
           sin(t) \\
\end{bmatrix}
\end{align}$$

#### Part b
$$
\begin{align}
&E(t):= x(t)^2+ y(t)^2\\
&\ii E(t) = sin^2(t) + cos^t(t)\\
&\ii E(t) = 1 \space \forall t
\end{align}
$$

#### Part c
We have the following:
1. $u(t) = \begin{bmatrix}x(t)\\y(t)\end{bmatrix} = \begin{bmatrix}cos(t)\\sin(t)\end{bmatrix}$
2. $u_k = \begin{bmatrix}x_k\\y_k\end{bmatrix}$
3. $u_{k+1} = u_k + hf(t_k, u_k) = \begin{bmatrix}x_k\\y_k\end{bmatrix} + h\begin{bmatrix}-sin(t_k)\\cos(t_k)\end{bmatrix} = \begin{bmatrix}x_k - hsin(t_k)\\y_k + hcos(t_k)\end{bmatrix}$ 
4. $E_k = x_k^2 + y_k^2$
5. $E_{k+1} = (x_k-hsin(t_k))^2 + (y_k+hcos(t_k))^2$

Now
$$\begin{align}
\tns {u_{k+1}} &= \sqrt{(x_k-hsin(t_k))^2 + (y_k+hcos(t_k))^2}^2\\
&= E_{k+1}\\
&= |(x_k-hsin(t_k))^2 + (y_k+hcos(t_k))^2|\\
&= |x_k^2 -2hsin(t_k)x_k + h^2sin(t_k)^2 + y_k^2 +2hcos(t_k)y_k + h^2cos(t_k)^2|\\
&= |x_k^2 + y_k^2 + h^2(sin^2(t_k) + cos^2(t_k)) + 2h(cos(t_k)y_k - sin(t_k)x_k)|\\
&=|E_k + h^2 + 2h(cos(t_k)y_k - sin (t_k)x_k)|\\
&≤ E_k + h^2 + 2h|cos(t_k)y_k - sin (t_k)x_k|\\
&≤ E_k + h^2 + 2h(|cos(t_k)y_k| + |sin (t_k)x_k|)\\
&≤ E_k + h^2 + 2h(|y_k| + |x_k|)\\
&= E_k + h^2 + 2h\lVert u_k\rVert_1\\
&≤ E_k + h^2 + 2h\sqrt 2\lVert u_k\rVert_2\\
&= E_k + h^2 + 2h\sqrt 2\sqrt E_k\\
\implies E_k+1&≤E_k + 2\sqrt 2h\sqrt E_k+ h^2 

\end{align}$$

#### Part d 
$$
\newcommand{\trth}{2\sqrt 2h}
\begin{align}
E_{k+1}&\le E_k+\trth\sqrt E_k + h^2 \\
&\le E_k + \trth E_k + h^2 \space (as \space E_k > 0)\\
&=(1+\trth)E_k + h^2\\
\end{align}$$
Then, by the lemma on slide 19 of ODEs lecture:
$$
\newcommand{\trt}{2\sqrt 2}
\begin{align}
E_n &= (1+\trth)^nE_0 + h^2(\frac{(1+\trth)^n - 1}{1 + \trth - 1})\\
&=(1+\trth)^nE_0 + h^2(\frac{(1+\trth)^n - 1}{\trth})\\
h = \frac{T}{n}&\implies(1+\trth) = (1+\trt \frac{T}{n})\\
1 + x \leq e^x&\implies (1+ \trt \frac{T}{n})\leq e^{\trt \frac{T}{n}}\\
\implies E_n &=(e^{\trt \frac{T}{n}})^nE_0 + h\frac{(e^{\trt \frac{T}{n}})^n-1}{\trt}\\
&=e^{\trt T}E_0 + \frac T n\frac{e^{\trt T}-1}{\trt}
\end{align}$$
## Question 4

#### Part a
We have: 
$$
\begin{align}
f(x) &= (x-1)^2\\
f'(x)&=2(x-1)\\
f''(x)&=2\\
f^{(k)}(x)&=0 &\forall k > 2
\end{align}
$$

For Newtons method
$$\begin{align}
x_{n+1} &= x_n - \frac{f(x_n)}{f'(x_n)}\\
\implies x_{n+1}&=x_n - \frac{(x_n-1)^2}{2(x_n-1)}\\
&=x_n - \frac1 2 (x_n-1)
\end{align}$$
Substituting $x_n = r - \epsilon_n$ where $r = 1$ is the true root of $f(x)$:
$$\begin{align}
r - \epsilon_{n + 1} &=r -\epsilon_n - \frac 1 2 (r - \epsilon_n - 1)\\
\implies \epsilon_{n+1} &=\epsilon_n +\frac 1 2 (1 - \epsilon_n - 1)\\
&= \epsilon_n -\frac 1 2 \epsilon_n\\
&=\frac 1 2 \epsilon_n\\
\\
\implies | \epsilon_{n + 1}| &= C | \epsilon_n| \quad for \space C = \frac 1 2
\end{align}$$
Thus Newton's method for $f(x) = (x+ 1)^2$ converges linearly.


#### Part b 
In the lectures, the following argument was made:

$$
\text{For some } \xi_k \text{ between }x_n \text{ and }r:\quad\quad
\begin{align}
e_{k+1}= e_k + \frac{f(x_k)}{f'(x_k)} = -\frac{f''(\xi_k)}{2f'(x_k)}e_k^2
\end{align}
$$
$$
\begin{align}
&x_k \text{ close to r} \implies \xi_k \text{ close to r} \\
&\space \implies \frac{f''(\xi_k)}{2f'(x_k)} \approx \frac {f''(r)} {2f'(r)}\\
& \space \implies e_{k+1} \approx -\frac {f''(r)} {2f'(r)}e_k^2
\end{align}
$$
However,
$$
\begin{align}
&f'(x) = 2(x-1) \space and \space r=1\\
&\space \implies f'(r) = f'(1) = 0\\
&\space\implies \frac{f''(\xi_k)}{2f'(x_k)} \not\approx \frac {f''(r)} {2f'(r)}
\end{align}
$$
As dividing by $f'(r)$ leads to a division by 0


#### Part c
$$\begin{align}
x_{n+1} &= x_n - 2\frac{f(x_n)}{f'(x_n)}\\
\implies x_{n+1}&=x_n - 2\frac{(x_n-1)^2}{2(x_n-1)}\\
&=x_n - (x_n-1)\\
&=1
\end{align}$$
Substituting $x_n = r - \epsilon_n$ where $r = 1$ is the true root of $f(x)$:
$$\begin{align}
r - \epsilon_{n + 1} &=r -\epsilon_n -  (r - \epsilon_n - 1)\\
\implies \epsilon_{n+1} &=\epsilon_n + (1 - \epsilon_n - 1)\\
&= \epsilon_n - \epsilon_n\\
&=0
\end{align}$$
Thus unlike regular Newtons method, this method will converge in a single iteration for any value of x.

## Question 5
#### Part a
$$
\begin{align}
&f(x) = \frac 1 2 x_1^2 + \frac c 2 x_2^2\\
&\nabla f(x) = \begin{bmatrix}x_1\\cx_2\end{bmatrix}\\
&\nabla f(x^*)=0\\
&\space\implies x_1=0 \space and \space x_2 = 0\\
&\space\implies x^*=\begin{bmatrix}0\\0\end{bmatrix}\\
& \space \implies x^*=(0,0)^T \text{ is a unique local minimum}
\end{align}
$$
#### Part b
$$\begin{align}
\nabla^2 f(x) &=\nabla\begin{bmatrix}x_1\\cx_2\end{bmatrix}\\
&= \begin{bmatrix}1&0\\0&c\end{bmatrix}\\
\\
\kappa(\nabla^2f(x)) &= \left\lVert \begin{bmatrix}1&0\\0&c\end{bmatrix}\right\lVert_2\left\lVert \begin{bmatrix}1&0\\0&c\end{bmatrix}^{-1}\right\lVert_2\\
&= \left\lVert \begin{bmatrix}1&0\\0&c\end{bmatrix}\right\lVert_2\left\lVert \frac 1 c\begin{bmatrix}c&0\\0&1\end{bmatrix}\right\lVert_2\\
&= \frac 1 c\left\lVert \begin{bmatrix}1&0\\0&c\end{bmatrix}\right\lVert_2\left\lVert \begin{bmatrix}c&0\\0&1\end{bmatrix}\right\lVert_2\\
\\
\left\lVert \begin{bmatrix}1&0\\0&c\end{bmatrix}\right\lVert_2 &= \sqrt{\lambda_{max}\left(\begin{bmatrix}1&0\\0&c\end{bmatrix}\begin{bmatrix}1&0\\0&c\end{bmatrix}^T\right)}\\
&=\sqrt{\lambda_{max}\begin{bmatrix}1&0\\0&c^2\end{bmatrix}}\\
&=\sqrt{\max \{\lambda \mid (\lambda^-1)(\lambda-c^2) = 0\}}\\
&=c\text{ as c > 1}\\
\\
\left\lVert \begin{bmatrix}c&0\\0&1\end{bmatrix}\right\lVert_2 &= \sqrt{\lambda_{max}\left(\begin{bmatrix}c&0\\0&1\end{bmatrix}\begin{bmatrix}c&0\\0&1\end{bmatrix}^T\right)}\\
&=\sqrt{\lambda_{max}\begin{bmatrix}c^2&0\\0&1\end{bmatrix}}\\
&=\sqrt{\max \{\lambda \mid (\lambda^-1)(\lambda-c^2) = 0\}}\\
&=c\text{ as c > 1}\\
\\
\implies \kappa(\nabla^2f(x)) &=\frac 1 c (c)(c)\\
&=c
\end{align}$$
#### Part c

$f(x)$ $L_{\nabla f }$-Lipschitz continuous if $\left\lVert\nabla f(y) - \nabla f(y)\right\lVert_2 \leq L_{\nabla f} \left\lVert y - x \right \rVert_2$
Further if $f(x)$ $L_{\nabla f }$-Lipschitz continuous then gradient descent converges globally if 0 < $\alpha$ < $\frac 2 {L_{\nabla f }}$
Thus, for $\alpha = \frac 1 c$, gradient descent converges if $f(x)$ is c-Lipschitz continuous as $\alpha < \frac 2 c$.
$$\begin{align}
\left\lVert\nabla f(y) - \nabla f(y)\right\lVert_2 &= \sqrt{(x_1 - y_1)^2 + c^2(x_2 - y_2)^2}\\
&= c \sqrt{\frac 1 {c^2}(x_1 - y_1)^2 + (x_2 - y_2)^2}\\
&< c\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2}\\
&= c \left\lVert y - x \right \rVert_2 
\end{align}$$
Thus $f(x)$ is c-Lipshitz continuous and thus converges globally to $x^*$ with step size $\frac 1 c$

Now, for gradient descent we have 
$$\begin{align}
x_{n+1} &≤ x_n -\alpha\nabla f(x_n)\\
\implies x_{n+1} &≤ x_n - \frac 1 c \nabla f(x_n)\\
&= \begin{bmatrix}x_{n_1}\\x_{n_2}\end{bmatrix} - \frac 1 c \begin{bmatrix}x_{n_1}\\cx_{n_2}\end{bmatrix}\\
&=\begin{bmatrix}x_{n_1}(1-\frac 1 c)\\0\end{bmatrix}
\end{align}$$
Thus, $x_{n_2}$ converges to $x^*_2$ after a single iteration. However, $x_{n+1_1} ≤ x_{n_1}(1 - \frac 1 c)$ which converges linearly as c > 1. Thus the overall convergence is linear.

#### Part d
For the convergence above we have $x_{n+1_1} ≤ x_{n_1}(1 - \frac 1 c)$. For larger values of c, the value of $(1-\frac 1 c)$ increases, approaching 1 and thus the value of $x_{n+1_1}$ will decrease more slowly. 

Furthermore, gradient descent performs poorly when $\nabla^2f(x)$ is ill conditioned. 
As $\kappa(\nabla^2f(x)) = c$ the hessian becomes more ill conditioned as c increases. Leading to slower convergence for larger values of c.  

## Question 6
#### Part a

$$\lVert x\rVert_2^2 ≤ \Delta^2 \Longleftrightarrow g(x) = \Delta^2 - \lVert x\rVert_2^2 ≥ 0$$

For Slater's CQ:
1. $g(x)\ concave$
2. $\exists x \in \mathbb{R}^n\ such\ that\ g(x) > 0$

For 1:
$$
\newcommand{\tns}[1]{\lVert {#1}\rVert_2^2}
\newcommand{\tn}[1]{\lVert {#1}\rVert_2}
\newcommand{\vec}{\mathbfit}
\begin{align}
-g(\vec{x})\ convex&\implies g(\vec{x})\ concave\\
-g(\vec{x}) &= -(\Delta^2 - \tns{\vec{x}} )\\
&=\tns{\vec{x}} - \Delta^2\\
&=\sum_{i=1}^nx_i^2 - \Delta^2\\
\\
\nabla (-g(\vec x)) &= \begin{bmatrix}2x_1\\2x_2\\\vdots\\2x_n\end{bmatrix}\\
\nabla^2(-g(\vec x)) &= 
    \begin{bmatrix}
	    2                                    \\
      & 2             &   & \text{\huge0}\\
      &               & 2                \\
      & \text{\huge0} &   & \ddots            \\
      &               &   &   & 2
    \end{bmatrix}\\
&\implies -g(x)\ convex\ as\ \nabla^2(-g(\vec x))\ positive\ semidefinate\\
&\implies g(x)\ concave
\end{align}
$$
For 2 consider $\vec x = \vec 0$:
$$\begin{align}
g(\vec 0)&=\Delta^2 - \tns {\vec 0}\\
&=\Delta^2 - 0\\
&=\Delta^2\\
&> 0\ as\ \Delta > 0
\end{align}$$

Thus Slater's CQ holds.

#### Part b
As $f(x)$ and $g(x)$ are continuously differentiable and Slater's CQ holds, local minimizer $x^*$ is a KKT point. 
$\newcommand{\xstar}{\vec{x}^*}\newcommand{\lstar}{\lambda^*}\newcommand{\star}{^*}$
Thus $\exists \lambda^*$ such that:
1. $\nabla f(\xstar) = \lstar \nabla g(\xstar)$
2. $\lstar ≥ 0$
3. $\lstar g(\xstar)=0$
4. $g(x) ≥ 0 \implies g(\xstar) ≥ 0 \implies \tns \xstar ≤ \Delta^2 \implies \tn \xstar ≤ \Delta$

From 3 we have:
$$\begin{align}
&\lstar = 0\quad or \quad g(\xstar)=0\\\\
&g(\xstar)=0\\
&\space \implies \Delta^2 - \tns\xstar = 0\\
& \space \implies (\Delta - \tn\xstar)(\Delta + \tn\xstar) = 0\\
&\space\implies \Delta - \tn\xstar = 0\quad as\ \Delta, \tn\xstar ≥ 0\\\\
Thus:\space &\lstar = 0 \quad or \quad \Delta - \tn\xstar = 0\\
&\space \implies \lstar(\Delta - \tn\xstar) = 0
\end{align}$$
Now: 
$$\begin{align}
&\nabla g(\vec x) = -2\begin{bmatrix}x_1\\x_2\\\vdots\\x_n\end{bmatrix} = -2 \vec x\\
&\space \implies \nabla g(\xstar) = -2\xstar\\\\
And: \space &f(x)= \frac 1 2 \vec x^T H \vec x + \vec x ^T \vec g\\
&\space \implies \nabla f(x)= \frac 1 2\nabla(\vec x^T H \vec x) + \nabla (\vec x^T \vec g)\end{align}$$
Considering $\vec x^T\vec y$:
$$\begin{align}
\nabla (\vec x^T \vec y) &=\nabla\left( \begin{bmatrix}x_1&x_2&\dots&x_n\end{bmatrix}\begin{bmatrix}y_1\\y_2\\\vdots\\y_n\end{bmatrix}\right)\\
&= \nabla\begin{bmatrix}x_1y_1\\x_2y_2\\\vdots\\x_ny_n\end{bmatrix}\\
&= \begin{bmatrix}y_1\\y_2\\\vdots\\y_n\end{bmatrix}\\
&=\vec y\\
\implies \nabla (\vec x ^T \vec g ) &= \vec g\\
\end{align}$$
Now let $\vec y = A\vec x$ for some symmetric matrix A:
$$\begin{align}
\nabla (\vec x^T\vec y) &= (\nabla \vec x^T )\vec y + \vec x^T (\nabla \vec y)\\
&=\vec y + \vec x^T \nabla (A\vec x)\\
&=\vec y + \vec x^T \nabla (A)\vec x + A \nabla \vec x\\
&=\vec y + \vec x^T A\\
&=A\vec x + \vec x^T A\\
&=A\vec x + A^T\vec x\\
&=(A+A^T)\vec x\\
&=2A\vec x\ as\ A\ symmetric\\
\implies \nabla(\vec x^T H \vec x)&= \frac 1 2 2H\vec x
\end{align}$$
Thus:
$$\begin{align}
&\nabla f(x) = \frac 1 2 2H\vec x + \vec g\\
&\space \implies f(\xstar) = H\xstar + g\\
\\
&\nabla f(x) = \lstar\nabla g(x)\\
&\space \implies H\xstar + \vec g= \lstar \xstar\\
&\space \implies H\xstar + \lstar \xstar= -\vec g \\
&\space \implies (H + \lstar I)\xstar = -g
\end{align}$$
